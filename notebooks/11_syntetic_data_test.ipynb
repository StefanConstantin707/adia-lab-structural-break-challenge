{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from src.data.dataLoader import StructuralBreakDataLoader\n",
    "# Structural Break Detection: Basic vs Synthetic Data Generation Approach\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, mannwhitneyu, levene, anderson_ksamp\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your existing data loader\n",
    "import sys\n",
    "sys.path.append('src')  # Adjust path as needed\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data_loader = StructuralBreakDataLoader()\n",
    "data_loader.load_data(use_crunch=False)\n",
    "train_data = data_loader.get_all_train_series()\n",
    "\n",
    "print(f\"Loaded {len(train_data)} time series\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPROACH 1: Basic Statistical Tests Between Pre/Post Break\n",
    "# =============================================================================\n",
    "\n",
    "def extract_basic_features(ts_data):\n",
    "    \"\"\"Extract basic statistical test features between pre and post break periods\"\"\"\n",
    "    pre_break = ts_data.period_0_values\n",
    "    post_break = ts_data.period_1_values\n",
    "    \n",
    "    if len(pre_break) < 5 or len(post_break) < 5:\n",
    "        # Return NaN features for very short series\n",
    "        return {\n",
    "            'ks_statistic': np.nan, 'ks_pvalue': np.nan,\n",
    "            'mw_statistic': np.nan, 'mw_pvalue': np.nan,\n",
    "            'levene_statistic': np.nan, 'levene_pvalue': np.nan,\n",
    "            'anderson_statistic': np.nan, 'anderson_pvalue': np.nan\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. Kolmogorov-Smirnov test (distribution difference)\n",
    "    try:\n",
    "        ks_stat, ks_pval = ks_2samp(pre_break, post_break)\n",
    "        features['ks_statistic'] = ks_stat\n",
    "        features['ks_pvalue'] = ks_pval\n",
    "    except:\n",
    "        features['ks_statistic'] = np.nan\n",
    "        features['ks_pvalue'] = np.nan\n",
    "    \n",
    "    # 2. Mann-Whitney U test (location difference)\n",
    "    try:\n",
    "        mw_stat, mw_pval = mannwhitneyu(pre_break, post_break, alternative='two-sided')\n",
    "        features['mw_statistic'] = mw_stat\n",
    "        features['mw_pvalue'] = mw_pval\n",
    "    except:\n",
    "        features['mw_statistic'] = np.nan\n",
    "        features['mw_pvalue'] = np.nan\n",
    "    \n",
    "    # 3. Levene's test (variance difference)\n",
    "    try:\n",
    "        levene_stat, levene_pval = levene(pre_break, post_break)\n",
    "        features['levene_statistic'] = levene_stat\n",
    "        features['levene_pvalue'] = levene_pval\n",
    "    except:\n",
    "        features['levene_statistic'] = np.nan\n",
    "        features['levene_pvalue'] = np.nan\n",
    "    \n",
    "    # 4. Anderson-Darling test (distribution difference, sensitive to tails)\n",
    "    try:\n",
    "        anderson_result = anderson_ksamp([pre_break, post_break])\n",
    "        features['anderson_statistic'] = anderson_result.statistic\n",
    "        features['anderson_pvalue'] = anderson_result.pvalue\n",
    "    except:\n",
    "        features['anderson_statistic'] = np.nan\n",
    "        features['anderson_pvalue'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extracting basic features...\")\n",
    "basic_features = []\n",
    "labels = []\n",
    "series_ids = []\n",
    "\n",
    "for series_id, ts_data in train_data.items():\n",
    "    features = extract_basic_features(ts_data)\n",
    "    basic_features.append(features)\n",
    "    labels.append(ts_data.has_break)\n",
    "    series_ids.append(series_id)\n",
    "\n",
    "basic_df = pd.DataFrame(basic_features, index=series_ids)\n",
    "y = pd.Series(labels, index=series_ids)\n",
    "\n",
    "# Remove rows with all NaN features\n",
    "basic_df_clean = basic_df.dropna()\n",
    "y_clean = y.loc[basic_df_clean.index]\n",
    "\n",
    "print(f\"Basic features shape: {basic_df_clean.shape}\")\n",
    "print(f\"Features: {list(basic_df_clean.columns)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPROACH 2: Synthetic Data Generation Method\n",
    "# =============================================================================\n",
    "\n",
    "def fit_ar_model(series, max_lags=10):\n",
    "    \"\"\"Fit AR model to time series, selecting optimal lag via AIC\"\"\"\n",
    "    if len(series) < max_lags + 5:\n",
    "        max_lags = max(1, len(series) - 5)\n",
    "    \n",
    "    best_aic = np.inf\n",
    "    best_model = None\n",
    "    best_lag = 1\n",
    "    \n",
    "    for lag in range(1, max_lags + 1):\n",
    "        try:\n",
    "            model = AutoReg(series, lags=lag, trend='c')\n",
    "            fitted = model.fit()\n",
    "            if fitted.aic < best_aic:\n",
    "                best_aic = fitted.aic\n",
    "                best_model = fitted\n",
    "                best_lag = lag\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return best_model, best_lag\n",
    "\n",
    "def generate_synthetic_continuations(pre_break_data, continuation_length, n_simulations=100):\n",
    "    \"\"\"Generate synthetic continuations using fitted AR model\"\"\"\n",
    "    \n",
    "    # Fit AR model\n",
    "    model, lag = fit_ar_model(pre_break_data)\n",
    "    \n",
    "    synthetic_continuations = []\n",
    "    for _ in range(n_simulations):\n",
    "        # Generate forecast\n",
    "        forecast = model.forecast(steps=continuation_length)\n",
    "        \n",
    "        # Add noise based on residual variance\n",
    "        noise = np.random.normal(0, np.sqrt(model.sigma2), continuation_length)\n",
    "        synthetic_series = forecast + noise\n",
    "        \n",
    "        synthetic_continuations.append(synthetic_series)\n",
    "    \n",
    "    return synthetic_continuations\n",
    "\n",
    "def extract_synthetic_features(ts_data, n_simulations=50):\n",
    "    \"\"\"Extract features using synthetic data generation approach\"\"\"\n",
    "    pre_break = ts_data.period_0_values\n",
    "    post_break = ts_data.period_1_values\n",
    "    \n",
    "    if len(pre_break) < 5 or len(post_break) < 5:\n",
    "        return {f'synth_{feat}': np.nan for feat in [\n",
    "            'ks_percentile', 'ks_zscore', 'mw_percentile', 'mw_zscore',\n",
    "            'levene_percentile', 'levene_zscore', 'anderson_percentile', 'anderson_zscore'\n",
    "        ]}\n",
    "    \n",
    "    # Generate synthetic continuations\n",
    "    synthetic_continuations = generate_synthetic_continuations(\n",
    "        pre_break, len(post_break), n_simulations\n",
    "    )\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Define test functions\n",
    "    test_functions = {\n",
    "        'ks': lambda x, y: ks_2samp(x, y)[0],  # statistic only\n",
    "        'mw': lambda x, y: mannwhitneyu(x, y, alternative='two-sided')[0],\n",
    "        'levene': lambda x, y: levene(x, y)[0],\n",
    "        'anderson': lambda x, y: anderson_ksamp([x, y]).statistic\n",
    "    }\n",
    "    \n",
    "    for test_name, test_func in test_functions.items():\n",
    "        try:\n",
    "            # Build null distribution of test statistics\n",
    "            null_stats = []\n",
    "            for synthetic in synthetic_continuations:\n",
    "                try:\n",
    "                    stat = test_func(pre_break, synthetic)\n",
    "                    null_stats.append(stat)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(null_stats) < 10:  # Need enough samples for meaningful distribution\n",
    "                features[f'synth_{test_name}_percentile'] = np.nan\n",
    "                features[f'synth_{test_name}_zscore'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            null_stats = np.array(null_stats)\n",
    "            \n",
    "            # Compute actual test statistic\n",
    "            actual_stat = test_func(pre_break, post_break)\n",
    "            \n",
    "            # Compute percentile (what fraction of null stats are <= actual)\n",
    "            percentile = np.mean(null_stats <= actual_stat)\n",
    "            features[f'synth_{test_name}_percentile'] = percentile\n",
    "            \n",
    "            # Compute z-score\n",
    "            if np.std(null_stats) > 0:\n",
    "                zscore = (actual_stat - np.mean(null_stats)) / np.std(null_stats)\n",
    "                features[f'synth_{test_name}_zscore'] = zscore\n",
    "            else:\n",
    "                features[f'synth_{test_name}_zscore'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            features[f'synth_{test_name}_percentile'] = np.nan\n",
    "            features[f'synth_{test_name}_zscore'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extracting synthetic features (this may take a few minutes)...\")\n",
    "synthetic_features = []\n",
    "\n",
    "# Process a subset for faster testing - remove this limit for full dataset\n",
    "subset_size = min(1000, len(train_data))  \n",
    "train_data_subset = dict(list(train_data.items())[:subset_size])\n",
    "\n",
    "for i, (series_id, ts_data) in enumerate(train_data_subset.items()):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Processing series {i+1}/{len(train_data_subset)}\")\n",
    "    \n",
    "    features = extract_synthetic_features(ts_data, n_simulations=30)  # Reduced for speed\n",
    "    synthetic_features.append(features)\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_features, index=list(train_data_subset.keys()))\n",
    "y_subset = pd.Series([ts.has_break for ts in train_data_subset.values()], \n",
    "                     index=list(train_data_subset.keys()))\n",
    "\n",
    "# Clean synthetic features\n",
    "synthetic_df_clean = synthetic_df.dropna()\n",
    "y_synthetic_clean = y_subset.loc[synthetic_df_clean.index]\n",
    "\n",
    "print(f\"Synthetic features shape: {synthetic_df_clean.shape}\")\n",
    "print(f\"Features: {list(synthetic_df_clean.columns)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION WITH CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_features(X, y, feature_type_name):\n",
    "    \"\"\"Evaluate features using cross-validation\"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(f\"No valid data for {feature_type_name}\")\n",
    "        return None\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        model, X, y,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    val_scores = cv_results['test_score']\n",
    "    train_scores = cv_results['train_score']\n",
    "    \n",
    "    results = {\n",
    "        'feature_type': feature_type_name,\n",
    "        'n_samples': len(X),\n",
    "        'n_features': X.shape[1],\n",
    "        'val_auc_mean': np.mean(val_scores),\n",
    "        'val_auc_std': np.std(val_scores),\n",
    "        'train_auc_mean': np.mean(train_scores),\n",
    "        'train_auc_std': np.std(train_scores),\n",
    "        'overfitting_gap': np.mean(train_scores) - np.mean(val_scores)\n",
    "    }\n",
    "    \n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T21:28:15.239577Z",
     "start_time": "2025-08-12T21:28:08.948352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate both approaches\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING BOTH APPROACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic features evaluation\n",
    "print(\"\\n1. Basic Statistical Tests Approach:\")\n",
    "basic_results = evaluate_features(basic_df_clean, y_clean, \"Basic Tests\")\n",
    "if basic_results:\n",
    "    print(f\"   Samples: {basic_results['n_samples']}\")\n",
    "    print(f\"   Features: {basic_results['n_features']}\")\n",
    "    print(f\"   CV ROC AUC: {basic_results['val_auc_mean']:.4f} ± {basic_results['val_auc_std']:.4f}\")\n",
    "    print(f\"   Train ROC AUC: {basic_results['train_auc_mean']:.4f} ± {basic_results['train_auc_std']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {basic_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# Synthetic features evaluation\n",
    "print(\"\\n2. Synthetic Data Generation Approach:\")\n",
    "synthetic_results = evaluate_features(synthetic_df_clean, y_synthetic_clean, \"Synthetic Tests\")\n",
    "if synthetic_results:\n",
    "    print(f\"   Samples: {synthetic_results['n_samples']}\")\n",
    "    print(f\"   Features: {synthetic_results['n_features']}\")\n",
    "    print(f\"   CV ROC AUC: {synthetic_results['val_auc_mean']:.4f} ± {synthetic_results['val_auc_std']:.4f}\")\n",
    "    print(f\"   Train ROC AUC: {synthetic_results['train_auc_mean']:.4f} ± {synthetic_results['train_auc_std']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {synthetic_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# Combined approach (if we have overlapping samples)\n",
    "common_indices = basic_df_clean.index.intersection(synthetic_df_clean.index)\n",
    "if len(common_indices) > 100:\n",
    "    print(\"\\n3. Combined Approach (Basic + Synthetic):\")\n",
    "    basic_subset = basic_df_clean.loc[common_indices]\n",
    "    synthetic_subset = synthetic_df_clean.loc[common_indices]\n",
    "    combined_features = pd.concat([basic_subset, synthetic_subset], axis=1)\n",
    "    y_combined = y_clean.loc[common_indices]\n",
    "    \n",
    "    combined_results = evaluate_features(combined_features, y_combined, \"Combined\")\n",
    "    if combined_results:\n",
    "        print(f\"   Samples: {combined_results['n_samples']}\")\n",
    "        print(f\"   Features: {combined_results['n_features']}\")\n",
    "        print(f\"   CV ROC AUC: {combined_results['val_auc_mean']:.4f} ± {combined_results['val_auc_std']:.4f}\")\n",
    "        print(f\"   Train ROC AUC: {combined_results['train_auc_mean']:.4f} ± {combined_results['train_auc_std']:.4f}\")\n",
    "        print(f\"   Overfitting Gap: {combined_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def get_feature_importance(X, y, top_n=10):\n",
    "    \"\"\"Get feature importance from trained model\"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df.head(top_n)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if synthetic_results:\n",
    "    print(f\"\\nTop 10 Most Important Synthetic Features:\")\n",
    "    synth_importance = get_feature_importance(synthetic_df_clean, y_synthetic_clean, 10)\n",
    "    for i, (_, row) in enumerate(synth_importance.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.6f}\")\n",
    "\n",
    "if basic_results:\n",
    "    print(f\"\\nTop 10 Most Important Basic Features:\")\n",
    "    basic_importance = get_feature_importance(basic_df_clean, y_clean, 10)\n",
    "    for i, (_, row) in enumerate(basic_importance.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.6f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if basic_results and synthetic_results:\n",
    "    improvement = synthetic_results['val_auc_mean'] - basic_results['val_auc_mean']\n",
    "    print(f\"Synthetic approach improves AUC by: {improvement:.4f}\")\n",
    "    if improvement > 0:\n",
    "        print(\"✅ Synthetic data generation method shows improvement!\")\n",
    "    else:\n",
    "        print(\"❌ Basic approach performs better (or try tuning synthetic parameters)\")\n",
    "else:\n",
    "    print(\"Could not compare both approaches - check data and feature extraction\")\n",
    "\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"- Basic tests directly compare pre/post break distributions\")\n",
    "print(f\"- Synthetic method creates null hypothesis of 'no break' scenario\")\n",
    "print(f\"- Percentile features show how extreme actual test stats are vs null\")\n",
    "print(f\"- Z-score features normalize for different null distribution scales\")"
   ],
   "id": "29d00666f116e5d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING BOTH APPROACHES\n",
      "============================================================\n",
      "\n",
      "1. Basic Statistical Tests Approach:\n",
      "   Samples: 10001\n",
      "   Features: 8\n",
      "   CV ROC AUC: 0.6062 ± 0.0206\n",
      "   Train ROC AUC: 0.8040 ± 0.0047\n",
      "   Overfitting Gap: 0.1978\n",
      "\n",
      "2. Synthetic Data Generation Approach:\n",
      "   Samples: 1000\n",
      "   Features: 8\n",
      "   CV ROC AUC: 0.5887 ± 0.0200\n",
      "   Train ROC AUC: 0.9827 ± 0.0011\n",
      "   Overfitting Gap: 0.3940\n",
      "\n",
      "3. Combined Approach (Basic + Synthetic):\n",
      "   Samples: 1000\n",
      "   Features: 16\n",
      "   CV ROC AUC: 0.5722 ± 0.0169\n",
      "   Train ROC AUC: 0.9991 ± 0.0005\n",
      "   Overfitting Gap: 0.4269\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Top 10 Most Important Synthetic Features:\n",
      " 1. synth_levene_zscore            0.161303\n",
      " 2. synth_ks_zscore                0.156033\n",
      " 3. synth_anderson_zscore          0.145570\n",
      " 4. synth_ks_percentile            0.122698\n",
      " 5. synth_mw_zscore                0.111069\n",
      " 6. synth_levene_percentile        0.106068\n",
      " 7. synth_anderson_percentile      0.105747\n",
      " 8. synth_mw_percentile            0.091512\n",
      "\n",
      "Top 10 Most Important Basic Features:\n",
      " 1. anderson_pvalue                0.233709\n",
      " 2. ks_statistic                   0.160230\n",
      " 3. anderson_statistic             0.144543\n",
      " 4. levene_pvalue                  0.118105\n",
      " 5. ks_pvalue                      0.100321\n",
      " 6. mw_pvalue                      0.084587\n",
      " 7. levene_statistic               0.084045\n",
      " 8. mw_statistic                   0.074459\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Synthetic approach improves AUC by: -0.0175\n",
      "❌ Basic approach performs better (or try tuning synthetic parameters)\n",
      "\n",
      "Key insights:\n",
      "- Basic tests directly compare pre/post break distributions\n",
      "- Synthetic method creates null hypothesis of 'no break' scenario\n",
      "- Percentile features show how extreme actual test stats are vs null\n",
      "- Z-score features normalize for different null distribution scales\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T21:51:09.225464Z",
     "start_time": "2025-08-12T21:47:37.471244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Structural Break Detection: GARCH-Based Synthetic Data Generation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, mannwhitneyu, levene, anderson_ksamp\n",
    "from arch import arch_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data.dataLoader import StructuralBreakDataLoader\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data_loader = StructuralBreakDataLoader()\n",
    "data_loader.load_data(use_crunch=False)\n",
    "train_data = data_loader.get_all_train_series()\n",
    "\n",
    "# Use same subset size for fair comparison\n",
    "SUBSET_SIZE = 1000\n",
    "np.random.seed(42)\n",
    "train_data_subset = dict(list(train_data.items())[:SUBSET_SIZE])\n",
    "\n",
    "print(f\"Using {SUBSET_SIZE} time series for comparison\")\n",
    "\n",
    "# =============================================================================\n",
    "# APPROACH 1: Basic Statistical Tests\n",
    "# =============================================================================\n",
    "\n",
    "def extract_basic_features(ts_data):\n",
    "    \"\"\"Extract basic statistical test features between pre and post break periods\"\"\"\n",
    "    pre_break = ts_data.period_0_values\n",
    "    post_break = ts_data.period_1_values\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Statistical tests\n",
    "    test_configs = [\n",
    "        ('ks', lambda: ks_2samp(pre_break, post_break)),\n",
    "        ('mw', lambda: mannwhitneyu(pre_break, post_break, alternative='two-sided')),\n",
    "        ('levene', lambda: levene(pre_break, post_break)),\n",
    "        ('anderson', lambda: anderson_ksamp([pre_break, post_break]))\n",
    "    ]\n",
    "    \n",
    "    for test_name, test_func in test_configs:\n",
    "        try:\n",
    "            if test_name == 'anderson':\n",
    "                result = test_func()\n",
    "                features[f'basic_{test_name}_statistic'] = result.statistic\n",
    "                features[f'basic_{test_name}_pvalue'] = result.pvalue\n",
    "            else:\n",
    "                stat, pval = test_func()\n",
    "                features[f'basic_{test_name}_statistic'] = stat\n",
    "                features[f'basic_{test_name}_pvalue'] = pval\n",
    "        except:\n",
    "            features[f'basic_{test_name}_statistic'] = np.nan\n",
    "            features[f'basic_{test_name}_pvalue'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# =============================================================================\n",
    "# APPROACH 2: GARCH-Based Synthetic Data Generation\n",
    "# =============================================================================\n",
    "\n",
    "class GARCHGenerator:\n",
    "    \"\"\"GARCH-based time series generator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted_model = None\n",
    "        self.mean_level = 0.0\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, series):\n",
    "        \"\"\"Fit GARCH model to time series\"\"\"\n",
    "        try:\n",
    "            # Store mean level\n",
    "            self.mean_level = np.mean(series)\n",
    "            \n",
    "            # Demean the series for GARCH (GARCH models the variance, not the mean)\n",
    "            demeaned_series = series - self.mean_level\n",
    "            \n",
    "            # Try GARCH(1,1) first - most common and robust\n",
    "            try:\n",
    "                garch_model = arch_model(demeaned_series, vol='GARCH', p=1, q=1, mean='Zero')\n",
    "                self.fitted_model = garch_model.fit(disp='off', show_warning=False)\n",
    "                self.is_fitted = True\n",
    "                return self\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try simpler ARCH(1) if GARCH fails\n",
    "            try:\n",
    "                arch_model_simple = arch_model(demeaned_series, vol='ARCH', p=1, mean='Zero')\n",
    "                self.fitted_model = arch_model_simple.fit(disp='off', show_warning=False)\n",
    "                self.is_fitted = True\n",
    "                return self\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try GARCH with constant mean\n",
    "            try:\n",
    "                garch_with_mean = arch_model(series, vol='GARCH', p=1, q=1, mean='Constant')\n",
    "                self.fitted_model = garch_with_mean.fit(disp='off', show_warning=False)\n",
    "                self.mean_level = 0  # Model handles mean internally\n",
    "                self.is_fitted = True\n",
    "                return self\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            self.is_fitted = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.is_fitted = False\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate_continuations(self, continuation_length, n_simulations=100):\n",
    "        \"\"\"Generate synthetic continuations using fitted GARCH model\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before generating continuations\")\n",
    "        \n",
    "        continuations = []\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            try:\n",
    "                # Generate simulation from GARCH model\n",
    "                sim_result = self.fitted_model.simulate(continuation_length, burn=100)\n",
    "                \n",
    "                # Extract the simulated data\n",
    "                if hasattr(sim_result, 'data'):\n",
    "                    simulated_series = sim_result['data'].values\n",
    "                else:\n",
    "                    simulated_series = sim_result.values\n",
    "                \n",
    "                # Add back mean level if we demeaned\n",
    "                continuation = simulated_series + self.mean_level\n",
    "                continuations.append(continuation)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If simulation fails, generate using residuals\n",
    "                try:\n",
    "                    residuals = self.fitted_model.resid\n",
    "                    if len(residuals) > 0:\n",
    "                        # Bootstrap from residuals and add mean\n",
    "                        continuation = np.random.choice(residuals, size=continuation_length, replace=True) + self.mean_level\n",
    "                        continuations.append(continuation)\n",
    "                    else:\n",
    "                        # Last resort: normal noise with estimated volatility\n",
    "                        volatility = np.sqrt(self.fitted_model.conditional_volatility.iloc[-1]) if hasattr(self.fitted_model, 'conditional_volatility') else 1.0\n",
    "                        continuation = np.random.normal(self.mean_level, volatility, continuation_length)\n",
    "                        continuations.append(continuation)\n",
    "                except:\n",
    "                    # Final fallback\n",
    "                    continuation = np.random.normal(self.mean_level, 1.0, continuation_length)\n",
    "                    continuations.append(continuation)\n",
    "        \n",
    "        return continuations\n",
    "\n",
    "def extract_garch_synthetic_features(ts_data, n_simulations=100):\n",
    "    \"\"\"Extract features using GARCH synthetic data generation\"\"\"\n",
    "    pre_break = ts_data.period_0_values\n",
    "    post_break = ts_data.period_1_values\n",
    "    \n",
    "    # Fit GARCH generator\n",
    "    generator = GARCHGenerator()\n",
    "    generator.fit(pre_break)\n",
    "    \n",
    "    if not generator.is_fitted:\n",
    "        # Return NaN features if GARCH fitting failed\n",
    "        return {f'garch_{feat}': np.nan for feat in [\n",
    "            'ks_percentile', 'ks_zscore', 'mw_percentile', 'mw_zscore',\n",
    "            'levene_percentile', 'levene_zscore', 'anderson_percentile', 'anderson_zscore',\n",
    "            'fit_success'\n",
    "        ]}\n",
    "    \n",
    "    # Generate synthetic continuations\n",
    "    try:\n",
    "        synthetic_continuations = generator.generate_continuations(len(post_break), n_simulations)\n",
    "    except:\n",
    "        return {f'garch_{feat}': np.nan for feat in [\n",
    "            'ks_percentile', 'ks_zscore', 'mw_percentile', 'mw_zscore',\n",
    "            'levene_percentile', 'levene_zscore', 'anderson_percentile', 'anderson_zscore',\n",
    "            'fit_success'\n",
    "        ]}\n",
    "    \n",
    "    features = {'garch_fit_success': 1.0}  # Successfully fitted and generated\n",
    "    \n",
    "    # Statistical tests with null distribution approach\n",
    "    test_functions = {\n",
    "        'ks': lambda x, y: ks_2samp(x, y)[0],\n",
    "        'mw': lambda x, y: mannwhitneyu(x, y, alternative='two-sided')[0],\n",
    "        'levene': lambda x, y: levene(x, y)[0],\n",
    "        'anderson': lambda x, y: anderson_ksamp([x, y]).statistic\n",
    "    }\n",
    "    \n",
    "    for test_name, test_func in test_functions.items():\n",
    "        try:\n",
    "            # Build null distribution of test statistics\n",
    "            null_stats = []\n",
    "            for synthetic in synthetic_continuations:\n",
    "                try:\n",
    "                    stat = test_func(pre_break, synthetic)\n",
    "                    if not np.isnan(stat) and not np.isinf(stat):\n",
    "                        null_stats.append(stat)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(null_stats) < 50:  # Need sufficient samples for stable distribution\n",
    "                features[f'garch_{test_name}_percentile'] = np.nan\n",
    "                features[f'garch_{test_name}_zscore'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            null_stats = np.array(null_stats)\n",
    "            \n",
    "            # Compute actual test statistic\n",
    "            actual_stat = test_func(pre_break, post_break)\n",
    "            \n",
    "            if np.isnan(actual_stat) or np.isinf(actual_stat):\n",
    "                features[f'garch_{test_name}_percentile'] = np.nan\n",
    "                features[f'garch_{test_name}_zscore'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            # Percentile: what fraction of null stats are <= actual\n",
    "            percentile = np.mean(null_stats <= actual_stat)\n",
    "            features[f'garch_{test_name}_percentile'] = percentile\n",
    "            \n",
    "            # Z-score: how many standard deviations from null mean\n",
    "            null_mean = np.mean(null_stats)\n",
    "            null_std = np.std(null_stats)\n",
    "            if null_std > 1e-8:\n",
    "                zscore = (actual_stat - null_mean) / null_std\n",
    "                features[f'garch_{test_name}_zscore'] = zscore\n",
    "            else:\n",
    "                features[f'garch_{test_name}_zscore'] = 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            features[f'garch_{test_name}_percentile'] = np.nan\n",
    "            features[f'garch_{test_name}_zscore'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT FEATURES FOR BOTH APPROACHES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Extracting basic features...\")\n",
    "basic_features = []\n",
    "labels = []\n",
    "series_ids = []\n",
    "\n",
    "for series_id, ts_data in train_data_subset.items():\n",
    "    features = extract_basic_features(ts_data)\n",
    "    basic_features.append(features)\n",
    "    labels.append(ts_data.has_break)\n",
    "    series_ids.append(series_id)\n",
    "\n",
    "basic_df = pd.DataFrame(basic_features, index=series_ids)\n",
    "y = pd.Series(labels, index=series_ids)\n",
    "\n",
    "print(\"Extracting GARCH synthetic features...\")\n",
    "garch_features = []\n",
    "\n",
    "for i, (series_id, ts_data) in enumerate(train_data_subset.items()):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing series {i+1}/{len(train_data_subset)}\")\n",
    "    \n",
    "    features = extract_garch_synthetic_features(ts_data, n_simulations=100)\n",
    "    garch_features.append(features)\n",
    "\n",
    "garch_df = pd.DataFrame(garch_features, index=series_ids)\n",
    "\n",
    "# Clean datasets - only remove rows where ALL features are NaN\n",
    "basic_df_clean = basic_df.dropna(how='all')\n",
    "y_basic_clean = y.loc[basic_df_clean.index]\n",
    "\n",
    "garch_df_clean = garch_df.dropna(how='all')\n",
    "y_garch_clean = y.loc[garch_df_clean.index]\n",
    "\n",
    "print(f\"Basic features shape: {basic_df_clean.shape}\")\n",
    "print(f\"GARCH features shape: {garch_df_clean.shape}\")\n",
    "\n",
    "# Check GARCH fitting success rate\n",
    "garch_success_rate = garch_df_clean['garch_fit_success'].mean()\n",
    "print(f\"GARCH fitting success rate: {garch_success_rate:.1%}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_features(X, y, feature_type_name):\n",
    "    \"\"\"Evaluate features using cross-validation\"\"\"\n",
    "    if len(X) == 0:\n",
    "        print(f\"No valid data for {feature_type_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Drop columns with too many NaNs (>50%)\n",
    "    nan_threshold = 0.5\n",
    "    valid_cols = X.columns[X.isnull().mean() < nan_threshold]\n",
    "    X_clean = X[valid_cols].fillna(X[valid_cols].median())\n",
    "    \n",
    "    if X_clean.shape[1] == 0:\n",
    "        print(f\"No valid features for {feature_type_name} after cleaning\")\n",
    "        return None\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        model, X_clean, y,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    val_scores = cv_results['test_score']\n",
    "    train_scores = cv_results['train_score']\n",
    "    \n",
    "    return {\n",
    "        'feature_type': feature_type_name,\n",
    "        'n_samples': len(X_clean),\n",
    "        'n_features': X_clean.shape[1],\n",
    "        'val_auc_mean': np.mean(val_scores),\n",
    "        'val_auc_std': np.std(val_scores),\n",
    "        'train_auc_mean': np.mean(train_scores),\n",
    "        'train_auc_std': np.std(train_scores),\n",
    "        'overfitting_gap': np.mean(train_scores) - np.mean(val_scores),\n",
    "        'clean_features': list(X_clean.columns)\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARING BASIC vs GARCH APPROACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic features\n",
    "basic_results = evaluate_features(basic_df_clean, y_basic_clean, \"Basic Tests\")\n",
    "print(f\"\\n1. Basic Statistical Tests:\")\n",
    "print(f\"   Samples: {basic_results['n_samples']}\")\n",
    "print(f\"   Features: {basic_results['n_features']}\")\n",
    "print(f\"   CV ROC AUC: {basic_results['val_auc_mean']:.4f} ± {basic_results['val_auc_std']:.4f}\")\n",
    "print(f\"   Train ROC AUC: {basic_results['train_auc_mean']:.4f} ± {basic_results['train_auc_std']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {basic_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# GARCH synthetic features\n",
    "garch_results = evaluate_features(garch_df_clean, y_garch_clean, \"GARCH Synthetic\")\n",
    "print(f\"\\n2. GARCH Synthetic Generation:\")\n",
    "print(f\"   Samples: {garch_results['n_samples']}\")\n",
    "print(f\"   Features: {garch_results['n_features']}\")\n",
    "print(f\"   CV ROC AUC: {garch_results['val_auc_mean']:.4f} ± {garch_results['val_auc_std']:.4f}\")\n",
    "print(f\"   Train ROC AUC: {garch_results['train_auc_mean']:.4f} ± {garch_results['train_auc_std']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {garch_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# Combined approach\n",
    "common_indices = basic_df_clean.index.intersection(garch_df_clean.index)\n",
    "if len(common_indices) > 100:\n",
    "    print(f\"\\n3. Combined Approach:\")\n",
    "    basic_subset = basic_df_clean.loc[common_indices]\n",
    "    garch_subset = garch_df_clean.loc[common_indices]\n",
    "    combined_features = pd.concat([basic_subset, garch_subset], axis=1)\n",
    "    y_combined = y.loc[common_indices]\n",
    "    \n",
    "    combined_results = evaluate_features(combined_features, y_combined, \"Combined\")\n",
    "    print(f\"   Samples: {combined_results['n_samples']}\")\n",
    "    print(f\"   Features: {combined_results['n_features']}\")\n",
    "    print(f\"   CV ROC AUC: {combined_results['val_auc_mean']:.4f} ± {combined_results['val_auc_std']:.4f}\")\n",
    "    print(f\"   Train ROC AUC: {combined_results['train_auc_mean']:.4f} ± {combined_results['train_auc_std']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {combined_results['overfitting_gap']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def get_feature_importance(X, y, top_n=10):\n",
    "    # Clean data for feature importance\n",
    "    nan_threshold = 0.5\n",
    "    valid_cols = X.columns[X.isnull().mean() < nan_threshold]\n",
    "    X_clean = X[valid_cols].fillna(X[valid_cols].median())\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_clean, y)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_clean.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    return importance_df.head(top_n)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTop 10 GARCH Synthetic Features:\")\n",
    "garch_importance = get_feature_importance(garch_df_clean, y_garch_clean, 10)\n",
    "for i, (_, row) in enumerate(garch_importance.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:<35} {row['importance']:.6f}\")\n",
    "\n",
    "print(f\"\\nTop 10 Basic Features:\")\n",
    "basic_importance = get_feature_importance(basic_df_clean, y_basic_clean, 10)\n",
    "for i, (_, row) in enumerate(basic_importance.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:<35} {row['importance']:.6f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improvement = garch_results['val_auc_mean'] - basic_results['val_auc_mean']\n",
    "print(f\"GARCH synthetic approach vs Basic: {improvement:+.4f} AUC improvement\")\n",
    "print(f\"GARCH fitting success rate: {garch_success_rate:.1%}\")\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print(\"✅ GARCH synthetic method shows meaningful improvement!\")\n",
    "elif improvement > 0:\n",
    "    print(\"🟡 Slight improvement with GARCH approach\")\n",
    "else:\n",
    "    print(\"❌ GARCH approach still not outperforming basic tests\")\n",
    "\n",
    "# Analyze what makes GARCH work well\n",
    "print(f\"\\nDiagnostics:\")\n",
    "print(f\"- Basic approach overfitting gap: {basic_results['overfitting_gap']:.3f}\")\n",
    "print(f\"- GARCH approach overfitting gap: {garch_results['overfitting_gap']:.3f}\")\n",
    "\n",
    "if garch_success_rate < 0.8:\n",
    "    print(f\"- Low GARCH success rate ({garch_success_rate:.1%}) may be limiting performance\")\n",
    "else:\n",
    "    print(f\"- Good GARCH success rate ({garch_success_rate:.1%})\")"
   ],
   "id": "375ba5c00f3b14da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.dataLoader:Data loaded successfully from local files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 time series for comparison\n",
      "Extracting basic features...\n",
      "Extracting GARCH synthetic features...\n",
      "Processing series 1/1000\n",
      "Processing series 101/1000\n",
      "Processing series 201/1000\n",
      "Processing series 301/1000\n",
      "Processing series 401/1000\n",
      "Processing series 501/1000\n",
      "Processing series 601/1000\n",
      "Processing series 701/1000\n",
      "Processing series 801/1000\n",
      "Processing series 901/1000\n",
      "Basic features shape: (1000, 8)\n",
      "GARCH features shape: (1000, 9)\n",
      "GARCH fitting success rate: 100.0%\n",
      "\n",
      "============================================================\n",
      "COMPARING BASIC vs GARCH APPROACHES\n",
      "============================================================\n",
      "\n",
      "1. Basic Statistical Tests:\n",
      "   Samples: 1000\n",
      "   Features: 8\n",
      "   CV ROC AUC: 0.5559 ± 0.0420\n",
      "   Train ROC AUC: 0.9903 ± 0.0030\n",
      "   Overfitting Gap: 0.4344\n",
      "\n",
      "2. GARCH Synthetic Generation:\n",
      "   Samples: 1000\n",
      "   Features: 9\n",
      "   CV ROC AUC: 0.5365 ± 0.0397\n",
      "   Train ROC AUC: 0.9818 ± 0.0029\n",
      "   Overfitting Gap: 0.4454\n",
      "\n",
      "3. Combined Approach:\n",
      "   Samples: 1000\n",
      "   Features: 17\n",
      "   CV ROC AUC: 0.5482 ± 0.0202\n",
      "   Train ROC AUC: 0.9975 ± 0.0013\n",
      "   Overfitting Gap: 0.4493\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Top 10 GARCH Synthetic Features:\n",
      " 1. garch_ks_zscore                     0.187575\n",
      " 2. garch_ks_percentile                 0.128897\n",
      " 3. garch_anderson_percentile           0.125107\n",
      " 4. garch_mw_zscore                     0.121628\n",
      " 5. garch_anderson_zscore               0.121555\n",
      " 6. garch_mw_percentile                 0.111312\n",
      " 7. garch_levene_percentile             0.109386\n",
      " 8. garch_levene_zscore                 0.094541\n",
      " 9. garch_fit_success                   0.000000\n",
      "\n",
      "Top 10 Basic Features:\n",
      " 1. basic_anderson_pvalue               0.198569\n",
      " 2. basic_ks_statistic                  0.151604\n",
      " 3. basic_ks_pvalue                     0.125311\n",
      " 4. basic_levene_pvalue                 0.122155\n",
      " 5. basic_anderson_statistic            0.103608\n",
      " 6. basic_mw_pvalue                     0.102313\n",
      " 7. basic_levene_statistic              0.098541\n",
      " 8. basic_mw_statistic                  0.097899\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "GARCH synthetic approach vs Basic: -0.0195 AUC improvement\n",
      "GARCH fitting success rate: 100.0%\n",
      "❌ GARCH approach still not outperforming basic tests\n",
      "\n",
      "Diagnostics:\n",
      "- Basic approach overfitting gap: 0.434\n",
      "- GARCH approach overfitting gap: 0.445\n",
      "- Good GARCH success rate (100.0%)\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
